{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils import getWordEmbedding\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument('w2v_model', type=str)\n",
    "#args = parser.parse_args()\n",
    "#\n",
    "#w2v_path = args.w2v_model\n",
    "w2v_path = 'word2vec_model/blogwiki_size200_alpha01_iter20.model'\n",
    "traindata_path = 'data/data_train.txt'\n",
    "testdata_path = 'data/data_test.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocab = 675933\n",
      "embedding_dim = 200\n",
      "2410\n",
      "6604\n",
      "29.939506664653464\n",
      "29.177828479926113\n",
      "53.53760289456816\n",
      "51.288254315522046\n"
     ]
    }
   ],
   "source": [
    "(word2id, id2word, embedding_matrix) = getWordEmbedding(w2v_path)\n",
    "wordemb_dim = embedding_matrix.shape[1]\n",
    "\n",
    "\n",
    "print('number of vocab = %d' %(len(word2id)), flush=True)\n",
    "print('embedding_dim = %d' %(wordemb_dim), flush=True)\n",
    "\n",
    "print(word2id['蘋果'])\n",
    "print(word2id['香蕉'])\n",
    "print(np.linalg.norm(embedding_matrix[word2id['蘋果']] - embedding_matrix[word2id['香蕉']]))\n",
    "print(np.linalg.norm(embedding_matrix[word2id['蘋果']] - embedding_matrix[word2id['鳳梨']]))\n",
    "print(np.linalg.norm(embedding_matrix[word2id['蘋果']] - embedding_matrix[word2id['天氣']]))\n",
    "print(np.linalg.norm(embedding_matrix[word2id['攝氏']] - embedding_matrix[word2id['蘋果']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1279, 909, 4268], [3, 1856, 148, 683], [193, 886], [1450, 205, 148, 3973, 342, 44], [178, 329, 571, 4354], [8714, 24889, 0, 6823, 880, 636], [55, 31, 22, 1087], [14956, 2, 10107], [713, 95838], [1450, 205, 5, 2651, 240, 44]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def getData(path, w2id):\n",
    "    x = []\n",
    "    y = []\n",
    "    fp = open(path, 'r', encoding='utf8')\n",
    "    for line in fp:\n",
    "        line_split = line.strip().split('\\t')\n",
    "        y.append(int(line_split[0]))\n",
    "        x.append(line_split[1:])\n",
    "    fp.close()\n",
    "    y = np.array(y)\n",
    "    # one hot\n",
    "    n_values = np.max(y) + 1\n",
    "    y = np.eye(n_values)[y]\n",
    "    # word to word_id\n",
    "    x = [[w2id[word] for word in _ if word in w2id] for _ in x]\n",
    "    return (x, y)\n",
    "\n",
    "train_x_id, train_y = getData(traindata_path, word2id)\n",
    "test_x_id, test_y = getData(testdata_path, word2id)\n",
    "\n",
    "print(train_x_id[0:10])\n",
    "print(train_y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _getSentVec(x, embedding_matrix):\n",
    "    dim = embedding_matrix.shape[1]\n",
    "    v_sum = np.zeros(dim)\n",
    "    for id in x:\n",
    "        v_sum += embedding_matrix[id]\n",
    "    return v_sum/len(x)\n",
    "\n",
    "train_x = np.array([_getSentVec(x, embedding_matrix) for x in train_x_id])\n",
    "test_x = np.array([_getSentVec(x, embedding_matrix) for x in test_x_id])\n",
    "train_data_cnt = train_x.shape[0]\n",
    "#print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.923957978776286\n",
      "16.2601450869681\n",
      "28.386265530580374\n",
      "29.780533865659823\n",
      "26.37557881399238\n",
      "=========================================\n",
      "23.77293741881347\n",
      "28.426846818552363\n",
      "15.943822633920009\n",
      "20.62106352144115\n",
      "22.406443381184456\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(train_x[20] - train_x[25]))\n",
    "print(np.linalg.norm(train_x[20] - train_x[34]))\n",
    "print(np.linalg.norm(train_x[0] - train_x[3]))\n",
    "print(np.linalg.norm(train_x[0] - train_x[4]))\n",
    "print(np.linalg.norm(train_x[0] - train_x[5]))\n",
    "print(\"=========================================\")\n",
    "print(np.linalg.norm(train_x[20] - train_x[18]))\n",
    "print(np.linalg.norm(train_x[20] - train_x[33]))\n",
    "print(np.linalg.norm(train_x[20] - train_x[913]))\n",
    "print(np.linalg.norm(train_x[1081] - train_x[3]))\n",
    "print(np.linalg.norm(train_x[1081] - train_x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_parameters = 242004\n",
      "Epoch: 010/1000 cost: 0.470147437020524\n",
      "Training accuracy: 0.805\n",
      "Test accuracy: 0.740\n",
      "\n",
      "Epoch: 020/1000 cost: 0.417251051288761\n",
      "Training accuracy: 0.809\n",
      "Test accuracy: 0.760\n",
      "\n",
      "Epoch: 030/1000 cost: 0.382682934607545\n",
      "Training accuracy: 0.815\n",
      "Test accuracy: 0.760\n",
      "\n",
      "Epoch: 040/1000 cost: 0.353547553085301\n",
      "Training accuracy: 0.825\n",
      "Test accuracy: 0.760\n",
      "\n",
      "Epoch: 050/1000 cost: 0.329386675806895\n",
      "Training accuracy: 0.835\n",
      "Test accuracy: 0.800\n",
      "\n",
      "Epoch: 060/1000 cost: 0.308738411493497\n",
      "Training accuracy: 0.860\n",
      "Test accuracy: 0.860\n",
      "\n",
      "Epoch: 070/1000 cost: 0.290122000730201\n",
      "Training accuracy: 0.894\n",
      "Test accuracy: 0.870\n",
      "\n",
      "Epoch: 080/1000 cost: 0.269885666362227\n",
      "Training accuracy: 0.917\n",
      "Test accuracy: 0.880\n",
      "\n",
      "Epoch: 090/1000 cost: 0.248735563599900\n",
      "Training accuracy: 0.931\n",
      "Test accuracy: 0.900\n",
      "\n",
      "Epoch: 100/1000 cost: 0.228889254674520\n",
      "Training accuracy: 0.950\n",
      "Test accuracy: 0.910\n",
      "\n",
      "Epoch: 110/1000 cost: 0.213220237664980\n",
      "Training accuracy: 0.962\n",
      "Test accuracy: 0.930\n",
      "\n",
      "Epoch: 120/1000 cost: 0.201658669604014\n",
      "Training accuracy: 0.969\n",
      "Test accuracy: 0.930\n",
      "\n",
      "Epoch: 130/1000 cost: 0.192993717242594\n",
      "Training accuracy: 0.974\n",
      "Test accuracy: 0.940\n",
      "\n",
      "Epoch: 140/1000 cost: 0.185870300212952\n",
      "Training accuracy: 0.976\n",
      "Test accuracy: 0.940\n",
      "\n",
      "Epoch: 150/1000 cost: 0.180353639672880\n",
      "Training accuracy: 0.978\n",
      "Test accuracy: 0.940\n",
      "\n",
      "Epoch: 160/1000 cost: 0.175435140728951\n",
      "Training accuracy: 0.980\n",
      "Test accuracy: 0.940\n",
      "\n",
      "Epoch: 170/1000 cost: 0.171434316733112\n",
      "Training accuracy: 0.982\n",
      "Test accuracy: 0.960\n",
      "\n",
      "Epoch: 180/1000 cost: 0.168132826685906\n",
      "Training accuracy: 0.983\n",
      "Test accuracy: 0.960\n",
      "\n",
      "Epoch: 190/1000 cost: 0.165163910143996\n",
      "Training accuracy: 0.983\n",
      "Test accuracy: 0.960\n",
      "\n",
      "Epoch: 200/1000 cost: 0.162513656975472\n",
      "Training accuracy: 0.983\n",
      "Test accuracy: 0.960\n",
      "\n",
      "Epoch: 210/1000 cost: 0.160241414626984\n",
      "Training accuracy: 0.986\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 220/1000 cost: 0.158195523978913\n",
      "Training accuracy: 0.987\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 230/1000 cost: 0.156304031814614\n",
      "Training accuracy: 0.987\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 240/1000 cost: 0.154660724409639\n",
      "Training accuracy: 0.987\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 250/1000 cost: 0.153153905517434\n",
      "Training accuracy: 0.990\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 260/1000 cost: 0.151794962687035\n",
      "Training accuracy: 0.991\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 270/1000 cost: 0.150567164159801\n",
      "Training accuracy: 0.991\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 280/1000 cost: 0.149441014209839\n",
      "Training accuracy: 0.991\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 290/1000 cost: 0.148430360098408\n",
      "Training accuracy: 0.991\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 300/1000 cost: 0.147464678507962\n",
      "Training accuracy: 0.991\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 310/1000 cost: 0.146571441260103\n",
      "Training accuracy: 0.992\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 320/1000 cost: 0.145777327965384\n",
      "Training accuracy: 0.992\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 330/1000 cost: 0.145031846549413\n",
      "Training accuracy: 0.992\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 340/1000 cost: 0.144329074106804\n",
      "Training accuracy: 0.993\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 350/1000 cost: 0.143676624722677\n",
      "Training accuracy: 0.993\n",
      "Test accuracy: 0.970\n",
      "\n",
      "Epoch: 360/1000 cost: 0.143077024450041\n",
      "Training accuracy: 0.993\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 370/1000 cost: 0.142497669344079\n",
      "Training accuracy: 0.994\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 380/1000 cost: 0.141944256343254\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 390/1000 cost: 0.141425870665132\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 400/1000 cost: 0.140927541541727\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 410/1000 cost: 0.140446836409503\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 420/1000 cost: 0.140012451026538\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 430/1000 cost: 0.139604601345650\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 440/1000 cost: 0.139221124453087\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 450/1000 cost: 0.138829351696250\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 460/1000 cost: 0.138461883958072\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 470/1000 cost: 0.138100128059518\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 480/1000 cost: 0.137724285868749\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 490/1000 cost: 0.137392678081173\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 500/1000 cost: 0.137072758519486\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 510/1000 cost: 0.136773490538336\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 520/1000 cost: 0.136462944011166\n",
      "Training accuracy: 0.995\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 530/1000 cost: 0.136134434848616\n",
      "Training accuracy: 0.996\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 540/1000 cost: 0.135849977601064\n",
      "Training accuracy: 0.996\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 550/1000 cost: 0.135582296407386\n",
      "Training accuracy: 0.996\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 560/1000 cost: 0.135320347670006\n",
      "Training accuracy: 0.996\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 570/1000 cost: 0.135036798168535\n",
      "Training accuracy: 0.997\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 580/1000 cost: 0.134785530297724\n",
      "Training accuracy: 0.997\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 590/1000 cost: 0.134521801790146\n",
      "Training accuracy: 0.997\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 600/1000 cost: 0.134251586174312\n",
      "Training accuracy: 0.997\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 610/1000 cost: 0.134001060912054\n",
      "Training accuracy: 0.997\n",
      "Test accuracy: 0.980\n",
      "\n",
      "Epoch: 620/1000 cost: 0.133784309233705\n",
      "Training accuracy: 0.997\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 630/1000 cost: 0.133563458919525\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 640/1000 cost: 0.133353938999241\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 650/1000 cost: 0.133167428308970\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 660/1000 cost: 0.132998893113985\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 670/1000 cost: 0.132830660637111\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 680/1000 cost: 0.132675641611831\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 690/1000 cost: 0.132533093225466\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 700/1000 cost: 0.132383424941808\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 710/1000 cost: 0.132240443401141\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 720/1000 cost: 0.132113498163550\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 730/1000 cost: 0.131972993481649\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 740/1000 cost: 0.131838754431842\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 750/1000 cost: 0.131720751932223\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 760/1000 cost: 0.131604187700846\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 770/1000 cost: 0.131484475650199\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 780/1000 cost: 0.131357618390697\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 790/1000 cost: 0.131254144319116\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 800/1000 cost: 0.131126896567541\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 810/1000 cost: 0.131027076138209\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 820/1000 cost: 0.130904867632748\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 830/1000 cost: 0.130766772652326\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 840/1000 cost: 0.130670259668402\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 850/1000 cost: 0.130578533427356\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 860/1000 cost: 0.130500736914269\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 870/1000 cost: 0.130386466849340\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 880/1000 cost: 0.130304197130138\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 890/1000 cost: 0.130218837971557\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 900/1000 cost: 0.130160833874794\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 910/1000 cost: 0.130094632302245\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 920/1000 cost: 0.130032358920738\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 930/1000 cost: 0.129969645036410\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 940/1000 cost: 0.129908275726723\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 950/1000 cost: 0.129856348445971\n",
      "Training accuracy: 0.998\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 960/1000 cost: 0.129795667039205\n",
      "Training accuracy: 0.999\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 970/1000 cost: 0.129758075287897\n",
      "Training accuracy: 0.999\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 980/1000 cost: 0.129697406332787\n",
      "Training accuracy: 0.999\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 990/1000 cost: 0.129656375679251\n",
      "Training accuracy: 0.999\n",
      "Test accuracy: 0.990\n",
      "\n",
      "Epoch: 1000/1000 cost: 0.129614419316592\n",
      "Training accuracy: 0.999\n",
      "Test accuracy: 0.990\n",
      "\n",
      "End of training.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STDDEV = 0.1\n",
    "BATCH_SIZE = 16\n",
    "TRAINING_EPOCHS = 1000\n",
    "LEARNING_RATE = 0.000001\n",
    "H1_SIZE = 200\n",
    "H2_SIZE = 200\n",
    "H3_SIZE = 200\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, wordemb_dim])\n",
    "y = tf.placeholder(tf.float32, [None, 2])\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "def mlp(_X, _weights, _biases, dropout_keep_prob):\n",
    "    layer1 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])), dropout_keep_prob)\n",
    "    layer2 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(layer1, _weights['h2']), _biases['b2'])), dropout_keep_prob)\n",
    "    layer3 = tf.nn.dropout(tf.nn.tanh(tf.add(tf.matmul(layer2, _weights['h3']), _biases['b3'])), dropout_keep_prob)\n",
    "    out = tf.nn.tanh(tf.add(tf.matmul(layer3, _weights['out']), _biases['out']))\n",
    "    return out\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([wordemb_dim, H1_SIZE],stddev=STDDEV)),\n",
    "    'h2': tf.Variable(tf.random_normal([H1_SIZE, H2_SIZE],stddev=STDDEV)),\n",
    "    'h3': tf.Variable(tf.random_normal([H2_SIZE, H3_SIZE],stddev=STDDEV)),\n",
    "    'out': tf.Variable(tf.random_normal([H3_SIZE, 2],stddev=STDDEV)),                                   \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([H1_SIZE])),\n",
    "    'b2': tf.Variable(tf.random_normal([H2_SIZE])),\n",
    "    'b3': tf.Variable(tf.random_normal([H3_SIZE])),\n",
    "    'out': tf.Variable(tf.random_normal([2]))\n",
    "}\n",
    "\n",
    "\n",
    "pred = mlp(X, weights, biases, dropout_keep_prob)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "predict_label = tf.argmax(pred, 1)\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "total_parameters = 0\n",
    "#iterating over all variables\n",
    "for variable in tf.trainable_variables():  \n",
    "    local_parameters=1\n",
    "    shape = variable.get_shape()  #getting shape of a variable\n",
    "    for i in shape:\n",
    "        local_parameters*=i.value  #mutiplying dimension values\n",
    "    total_parameters+=local_parameters\n",
    "print('total_parameters = %d' %(total_parameters))\n",
    "\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    epoch += 1\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(train_data_cnt / BATCH_SIZE)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        idx_start = i*BATCH_SIZE\n",
    "        idx_end = i*BATCH_SIZE + BATCH_SIZE\n",
    "        if(idx_end >= train_data_cnt):\n",
    "            idx_end = train_data_cnt\n",
    "        batch_xs = train_x[idx_start:idx_end, :]\n",
    "        batch_ys = train_y[idx_start:idx_end, :]\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={X: batch_xs, y: batch_ys, dropout_keep_prob: 0.9})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={X: batch_xs, y: batch_ys, dropout_keep_prob:1.})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % 10 == 0 or epoch == (TRAINING_EPOCHS):\n",
    "        print (\"Epoch: %03d/%03d cost: %.15f\" % (epoch, TRAINING_EPOCHS, avg_cost))\n",
    "        train_acc = sess.run(accuracy, feed_dict={X: train_x, y: train_y, dropout_keep_prob:1.})\n",
    "        [test_acc, test_pred] = sess.run([accuracy, predict_label], feed_dict={X: test_x, y: test_y, dropout_keep_prob:1.})\n",
    "        print (\"Training accuracy: %.3f\" % (train_acc))\n",
    "        print (\"Test accuracy: %.3f\" % (test_acc))\n",
    "        print('')\n",
    "        \n",
    "        # write test result\n",
    "        fp = open('test_epoch{}.txt'.format(epoch), 'w', encoding='utf8')\n",
    "        fp.write('target, predict, sentence\\n')\n",
    "        for i in range(len(test_y)):\n",
    "            fp.write('{}, {}, {}\\n'.format(np.argmax(test_y[i]), test_pred[i], ''.join([id2word[w] for w in test_x_id[i]])))\n",
    "        fp.close()\n",
    "        \n",
    "\n",
    "\n",
    "print (\"End of training.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
